{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbjKhCzfI6bk",
        "outputId": "d4de591c-1ae7-4863-f6a6-530890657bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating mock market data...\n",
            "\n",
            "âœ“ Generated 202,534 ticks across 4 symbols and 25 days\n",
            "âœ“ Data shape: (202534, 11)\n",
            "âœ“ Date range: 2024-10-01 to 2024-11-04\n",
            "\n",
            "Sample data:\n",
            "  symbol        date           timestamp   price  volume     bid     ask  \\\n",
            "0   AAPL  2024-10-01 2024-10-01 09:30:00  180.00     458  180.00  180.02   \n",
            "1   AAPL  2024-10-01 2024-10-01 09:30:15  179.99    1996  179.97  180.00   \n",
            "2   AAPL  2024-10-01 2024-10-01 09:30:30  180.01    1002  179.99  180.01   \n",
            "3   AAPL  2024-10-01 2024-10-01 09:30:45  179.97    1378  179.97  179.99   \n",
            "4   AAPL  2024-10-01 2024-10-01 09:31:00  179.96    1028  179.96  179.98   \n",
            "5   AAPL  2024-10-01 2024-10-01 09:31:05  179.98     456  179.96  179.98   \n",
            "6   AAPL  2024-10-01 2024-10-01 09:31:10  180.06     277  180.05  180.07   \n",
            "7   AAPL  2024-10-01 2024-10-01 09:31:15  180.05    1582  180.05  180.07   \n",
            "8   AAPL  2024-10-01 2024-10-01 09:31:20  180.04     630  180.04  180.06   \n",
            "9   AAPL  2024-10-01 2024-10-01 09:31:25  180.01     264  180.00  180.02   \n",
            "\n",
            "   bid_size  ask_size  trade_direction  quote_update  \n",
            "0      2408      4572               -1             0  \n",
            "1       872       799                1             1  \n",
            "2      2035      1331                1             0  \n",
            "3      1160     25833               -1             0  \n",
            "4      1699      8538               -1             0  \n",
            "5       334      5152                1             0  \n",
            "6      3615       829                1             0  \n",
            "7      3823      1216               -1             0  \n",
            "8      4531      1549               -1             1  \n",
            "9      2333     14856                1             1  \n",
            "\n",
            "\n",
            "================================================================================\n",
            "ANALYZING AAPL - 5 trade days\n",
            "================================================================================\n",
            "\n",
            "âš ï¸ Warning: Only 10 non-trading days available for AAPL\n",
            "================================================================================\n",
            "VWAP FOOTPRINT ANALYSIS - AAPL - 2024-10-15\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š COMPOSITE DETECTABILITY SCORE: 105.2/100\n",
            "\n",
            "ðŸš¨ HIGH RISK: Footprint is highly detectable - likely exploitable!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "METRIC-BY-METRIC ANALYSIS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ðŸš¨ SPREAD\n",
            "   Correlation:    -0.213\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   4.53\n",
            "   Euclidean dist: 0.0220\n",
            "\n",
            "ðŸš¨ TRADE_IMBALANCE\n",
            "   Correlation:    -0.146\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   0.78\n",
            "   Euclidean dist: 1.3470\n",
            "\n",
            "ðŸš¨ PRICE_RETURN\n",
            "   Correlation:    -0.059\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   0.89\n",
            "   Euclidean dist: 8.7031\n",
            "\n",
            "ðŸš¨ VOLATILITY\n",
            "   Correlation:    -0.043\n",
            "   KS p-value:     0.0006\n",
            "   Mean Z-score:   1.06\n",
            "   Euclidean dist: 0.0472\n",
            "\n",
            "ðŸš¨ DEPTH_IMBALANCE\n",
            "   Correlation:    -0.032\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   0.80\n",
            "   Euclidean dist: 0.8333\n",
            "\n",
            "ðŸš¨ VOLUME\n",
            "   Correlation:    -0.007\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   2.95\n",
            "   Euclidean dist: 0.0583\n",
            "\n",
            "ðŸš¨ TRADE_COUNT\n",
            "   Correlation:    0.009\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   1.71\n",
            "   Euclidean dist: 0.0322\n",
            "\n",
            "ðŸš¨ PRICE_IMPACT\n",
            "   Correlation:    0.025\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   0.77\n",
            "   Euclidean dist: 0.1006\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ðŸ” SUSPICIOUS METRICS (requiring attention)\n",
            "--------------------------------------------------------------------------------\n",
            "  â€¢ volume: correlation=-0.007, p-value=0.0000\n",
            "  â€¢ trade_count: correlation=0.009, p-value=0.0000\n",
            "  â€¢ price_return: correlation=-0.059, p-value=0.0000\n",
            "  â€¢ volatility: correlation=-0.043, p-value=0.0006\n",
            "  â€¢ spread: correlation=-0.213, p-value=0.0000\n",
            "  â€¢ depth_imbalance: correlation=-0.032, p-value=0.0000\n",
            "  â€¢ trade_imbalance: correlation=-0.146, p-value=0.0000\n",
            "  â€¢ price_impact: correlation=0.025, p-value=0.0000\n",
            "\n",
            "================================================================================\n",
            "\n",
            "âš ï¸ Warning: Only 14 non-trading days available for AAPL\n",
            "================================================================================\n",
            "VWAP FOOTPRINT ANALYSIS - AAPL - 2024-10-22\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š COMPOSITE DETECTABILITY SCORE: 105.4/100\n",
            "\n",
            "ðŸš¨ HIGH RISK: Footprint is highly detectable - likely exploitable!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "METRIC-BY-METRIC ANALYSIS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ðŸš¨ SPREAD\n",
            "   Correlation:    -0.202\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   5.01\n",
            "   Euclidean dist: 0.0227\n",
            "\n",
            "ðŸš¨ DEPTH_IMBALANCE\n",
            "   Correlation:    -0.109\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   0.81\n",
            "   Euclidean dist: 0.8875\n",
            "\n",
            "ðŸš¨ PRICE_IMPACT\n",
            "   Correlation:    -0.091\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   0.87\n",
            "   Euclidean dist: 0.1042\n",
            "\n",
            "ðŸš¨ TRADE_COUNT\n",
            "   Correlation:    -0.073\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   1.59\n",
            "   Euclidean dist: 0.0315\n",
            "\n",
            "ðŸš¨ PRICE_RETURN\n",
            "   Correlation:    -0.049\n",
            "   KS p-value:     0.0003\n",
            "   Mean Z-score:   1.00\n",
            "   Euclidean dist: 4.4032\n",
            "\n",
            "ðŸš¨ TRADE_IMBALANCE\n",
            "   Correlation:    -0.039\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   0.84\n",
            "   Euclidean dist: 16.6154\n",
            "\n",
            "ðŸš¨ VOLUME\n",
            "   Correlation:    -0.004\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   2.55\n",
            "   Euclidean dist: 0.0526\n",
            "\n",
            "ðŸš¨ VOLATILITY\n",
            "   Correlation:    0.038\n",
            "   KS p-value:     0.0021\n",
            "   Mean Z-score:   0.99\n",
            "   Euclidean dist: 0.0483\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ðŸ” SUSPICIOUS METRICS (requiring attention)\n",
            "--------------------------------------------------------------------------------\n",
            "  â€¢ volume: correlation=-0.004, p-value=0.0000\n",
            "  â€¢ trade_count: correlation=-0.073, p-value=0.0000\n",
            "  â€¢ price_return: correlation=-0.049, p-value=0.0003\n",
            "  â€¢ volatility: correlation=0.038, p-value=0.0021\n",
            "  â€¢ spread: correlation=-0.202, p-value=0.0000\n",
            "  â€¢ depth_imbalance: correlation=-0.109, p-value=0.0000\n",
            "  â€¢ trade_imbalance: correlation=-0.039, p-value=0.0000\n",
            "  â€¢ price_impact: correlation=-0.091, p-value=0.0000\n",
            "\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "VWAP FOOTPRINT ANALYSIS - AAPL - 2024-10-28\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š COMPOSITE DETECTABILITY SCORE: 105.7/100\n",
            "\n",
            "ðŸš¨ HIGH RISK: Footprint is highly detectable - likely exploitable!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "METRIC-BY-METRIC ANALYSIS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ðŸš¨ SPREAD\n",
            "   Correlation:    -0.142\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   4.35\n",
            "   Euclidean dist: 0.0246\n",
            "\n",
            "ðŸš¨ VOLATILITY\n",
            "   Correlation:    -0.084\n",
            "   KS p-value:     0.0012\n",
            "   Mean Z-score:   0.86\n",
            "   Euclidean dist: 0.0468\n",
            "\n",
            "ðŸš¨ VOLUME\n",
            "   Correlation:    -0.078\n",
            "   KS p-value:     0.0002\n",
            "   Mean Z-score:   2.89\n",
            "   Euclidean dist: 0.0657\n",
            "\n",
            "ðŸš¨ TRADE_COUNT\n",
            "   Correlation:    -0.065\n",
            "   KS p-value:     0.0003\n",
            "   Mean Z-score:   1.59\n",
            "   Euclidean dist: 0.0342\n",
            "\n",
            "ðŸš¨ DEPTH_IMBALANCE\n",
            "   Correlation:    -0.043\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   0.76\n",
            "   Euclidean dist: 0.8910\n",
            "\n",
            "ðŸš¨ TRADE_IMBALANCE\n",
            "   Correlation:    -0.036\n",
            "   KS p-value:     0.0006\n",
            "   Mean Z-score:   0.85\n",
            "   Euclidean dist: 4.1955\n",
            "\n",
            "ðŸš¨ PRICE_IMPACT\n",
            "   Correlation:    -0.033\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   0.88\n",
            "   Euclidean dist: 0.1394\n",
            "\n",
            "ðŸš¨ PRICE_RETURN\n",
            "   Correlation:    0.019\n",
            "   KS p-value:     0.0002\n",
            "   Mean Z-score:   0.90\n",
            "   Euclidean dist: 2.6540\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ðŸ” SUSPICIOUS METRICS (requiring attention)\n",
            "--------------------------------------------------------------------------------\n",
            "  â€¢ volume: correlation=-0.078, p-value=0.0002\n",
            "  â€¢ trade_count: correlation=-0.065, p-value=0.0003\n",
            "  â€¢ price_return: correlation=0.019, p-value=0.0002\n",
            "  â€¢ volatility: correlation=-0.084, p-value=0.0012\n",
            "  â€¢ spread: correlation=-0.142, p-value=0.0000\n",
            "  â€¢ depth_imbalance: correlation=-0.043, p-value=0.0000\n",
            "  â€¢ trade_imbalance: correlation=-0.036, p-value=0.0006\n",
            "  â€¢ price_impact: correlation=-0.033, p-value=0.0000\n",
            "\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "VWAP FOOTPRINT ANALYSIS - AAPL - 2024-10-30\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š COMPOSITE DETECTABILITY SCORE: 80.4/100\n",
            "\n",
            "ðŸš¨ HIGH RISK: Footprint is highly detectable - likely exploitable!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "METRIC-BY-METRIC ANALYSIS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ðŸš¨ DEPTH_IMBALANCE\n",
            "   Correlation:    -0.083\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   0.91\n",
            "   Euclidean dist: 0.5776\n",
            "\n",
            "ðŸš¨ TRADE_IMBALANCE\n",
            "   Correlation:    -0.071\n",
            "   KS p-value:     0.0002\n",
            "   Mean Z-score:   0.92\n",
            "   Euclidean dist: 7.2052\n",
            "\n",
            "ðŸš¨ PRICE_IMPACT\n",
            "   Correlation:    0.038\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   0.87\n",
            "   Euclidean dist: 0.1007\n",
            "\n",
            "ðŸš¨ VOLATILITY\n",
            "   Correlation:    0.155\n",
            "   KS p-value:     0.0116\n",
            "   Mean Z-score:   0.95\n",
            "   Euclidean dist: 0.0519\n",
            "\n",
            "ðŸš¨ PRICE_RETURN\n",
            "   Correlation:    0.194\n",
            "   KS p-value:     0.0006\n",
            "   Mean Z-score:   0.89\n",
            "   Euclidean dist: 2.1737\n",
            "\n",
            "ðŸš¨ TRADE_COUNT\n",
            "   Correlation:    0.274\n",
            "   KS p-value:     0.0039\n",
            "   Mean Z-score:   1.30\n",
            "   Euclidean dist: 0.0306\n",
            "\n",
            "ðŸš¨ VOLUME\n",
            "   Correlation:    0.390\n",
            "   KS p-value:     0.0003\n",
            "   Mean Z-score:   1.77\n",
            "   Euclidean dist: 0.0494\n",
            "\n",
            "ðŸš¨ SPREAD\n",
            "   Correlation:    0.500\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   2.39\n",
            "   Euclidean dist: 0.0125\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ðŸ” SUSPICIOUS METRICS (requiring attention)\n",
            "--------------------------------------------------------------------------------\n",
            "  â€¢ volume: correlation=0.390, p-value=0.0003\n",
            "  â€¢ trade_count: correlation=0.274, p-value=0.0039\n",
            "  â€¢ price_return: correlation=0.194, p-value=0.0006\n",
            "  â€¢ volatility: correlation=0.155, p-value=0.0116\n",
            "  â€¢ spread: correlation=0.500, p-value=0.0000\n",
            "  â€¢ depth_imbalance: correlation=-0.083, p-value=0.0000\n",
            "  â€¢ trade_imbalance: correlation=-0.071, p-value=0.0002\n",
            "  â€¢ price_impact: correlation=0.038, p-value=0.0000\n",
            "\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "VWAP FOOTPRINT ANALYSIS - AAPL - 2024-11-01\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š COMPOSITE DETECTABILITY SCORE: 107.3/100\n",
            "\n",
            "ðŸš¨ HIGH RISK: Footprint is highly detectable - likely exploitable!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "METRIC-BY-METRIC ANALYSIS\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ðŸš¨ TRADE_COUNT\n",
            "   Correlation:    -0.245\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   1.73\n",
            "   Euclidean dist: 0.0375\n",
            "\n",
            "ðŸš¨ VOLUME\n",
            "   Correlation:    -0.172\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   2.99\n",
            "   Euclidean dist: 0.0715\n",
            "\n",
            "ðŸš¨ VOLATILITY\n",
            "   Correlation:    -0.131\n",
            "   KS p-value:     0.0006\n",
            "   Mean Z-score:   1.06\n",
            "   Euclidean dist: 0.0484\n",
            "\n",
            "ðŸš¨ TRADE_IMBALANCE\n",
            "   Correlation:    -0.082\n",
            "   KS p-value:     0.0001\n",
            "   Mean Z-score:   0.82\n",
            "   Euclidean dist: 1.2873\n",
            "\n",
            "ðŸš¨ SPREAD\n",
            "   Correlation:    -0.072\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   4.17\n",
            "   Euclidean dist: 0.0249\n",
            "\n",
            "ðŸš¨ PRICE_IMPACT\n",
            "   Correlation:    -0.021\n",
            "   KS p-value:     0.0000\n",
            "   Mean Z-score:   0.83\n",
            "   Euclidean dist: 0.1201\n",
            "\n",
            "ðŸš¨ PRICE_RETURN\n",
            "   Correlation:    0.065\n",
            "   KS p-value:     0.0012\n",
            "   Mean Z-score:   1.01\n",
            "   Euclidean dist: 2.3466\n",
            "\n",
            "ðŸš¨ DEPTH_IMBALANCE\n",
            "   Correlation:    0.130\n",
            "   KS p-value:     0.0002\n",
            "   Mean Z-score:   0.91\n",
            "   Euclidean dist: 11.0226\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "ðŸ” SUSPICIOUS METRICS (requiring attention)\n",
            "--------------------------------------------------------------------------------\n",
            "  â€¢ volume: correlation=-0.172, p-value=0.0000\n",
            "  â€¢ trade_count: correlation=-0.245, p-value=0.0000\n",
            "  â€¢ price_return: correlation=0.065, p-value=0.0012\n",
            "  â€¢ volatility: correlation=-0.131, p-value=0.0006\n",
            "  â€¢ spread: correlation=-0.072, p-value=0.0000\n",
            "  â€¢ depth_imbalance: correlation=0.130, p-value=0.0002\n",
            "  â€¢ trade_imbalance: correlation=-0.082, p-value=0.0001\n",
            "  â€¢ price_impact: correlation=-0.021, p-value=0.0000\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta, time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import euclidean, cosine\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# MOCK DATA GENERATOR\n",
        "# ============================================================================\n",
        "\n",
        "class MarketDataGenerator:\n",
        "    \"\"\"Generate realistic mock market microstructure data\"\"\"\n",
        "\n",
        "    def __init__(self, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.symbols = {\n",
        "            'AAPL': {'price': 180.0, 'volatility': 0.02, 'spread_bps': 1.0, 'avg_volume': 1000000},\n",
        "            'MSFT': {'price': 380.0, 'volatility': 0.018, 'spread_bps': 1.2, 'avg_volume': 800000},\n",
        "            'GOOGL': {'price': 140.0, 'volatility': 0.022, 'spread_bps': 1.5, 'avg_volume': 600000},\n",
        "            'TSLA': {'price': 240.0, 'volatility': 0.035, 'spread_bps': 2.0, 'avg_volume': 1200000},\n",
        "            'JPM': {'price': 155.0, 'volatility': 0.015, 'spread_bps': 0.8, 'avg_volume': 500000}\n",
        "        }\n",
        "\n",
        "    def generate_intraday_volume_curve(self, n_buckets=390):\n",
        "        \"\"\"\n",
        "        Generate realistic U-shaped intraday volume curve\n",
        "        High at open, low mid-day, high at close\n",
        "        \"\"\"\n",
        "        x = np.linspace(0, 1, n_buckets)\n",
        "        # U-shaped curve using polynomial\n",
        "        volume_curve = 2.5 * (x ** 2) - 2.5 * x + 1.5\n",
        "        # Add some randomness\n",
        "        volume_curve += np.random.normal(0, 0.1, n_buckets)\n",
        "        volume_curve = np.maximum(volume_curve, 0.3)  # Floor at 30% of mean\n",
        "        return volume_curve / volume_curve.mean()  # Normalize to mean=1\n",
        "\n",
        "    def generate_day_data(self, symbol, date, is_trade_day=False,\n",
        "                          trade_intensity=1.0, trade_start_pct=0.3, trade_end_pct=0.7):\n",
        "        \"\"\"\n",
        "        Generate one day of tick data for a symbol\n",
        "\n",
        "        is_trade_day: If True, adds detectable footprint\n",
        "        trade_intensity: How much extra volume on trade days (1.0 = normal, 2.0 = double)\n",
        "        trade_start_pct: When trading starts (0.3 = 30% into day)\n",
        "        trade_end_pct: When trading ends (0.7 = 70% into day)\n",
        "        \"\"\"\n",
        "        params = self.symbols[symbol]\n",
        "\n",
        "        # Market hours: 9:30 AM to 4:00 PM = 390 minutes\n",
        "        market_open = datetime.combine(date, time(9, 30))\n",
        "        n_minutes = 390\n",
        "\n",
        "        # Generate volume curve\n",
        "        base_volume_curve = self.generate_intraday_volume_curve(n_minutes)\n",
        "\n",
        "        # If trade day, add footprint in specified window\n",
        "        if is_trade_day:\n",
        "            trade_start_idx = int(n_minutes * trade_start_pct)\n",
        "            trade_end_idx = int(n_minutes * trade_end_pct)\n",
        "\n",
        "            # Add volume spike during trading window\n",
        "            volume_multiplier = np.ones(n_minutes)\n",
        "            volume_multiplier[trade_start_idx:trade_end_idx] *= trade_intensity\n",
        "\n",
        "            # Add some \"leakage\" - slight volume increase before/after\n",
        "            if trade_start_idx > 10:\n",
        "                volume_multiplier[trade_start_idx-10:trade_start_idx] *= 1.1\n",
        "            if trade_end_idx < n_minutes - 10:\n",
        "                volume_multiplier[trade_end_idx:trade_end_idx+10] *= 1.1\n",
        "\n",
        "            base_volume_curve *= volume_multiplier\n",
        "\n",
        "        # Generate tick data\n",
        "        ticks = []\n",
        "        current_price = params['price']\n",
        "\n",
        "        for minute in range(n_minutes):\n",
        "            timestamp = market_open + timedelta(minutes=minute)\n",
        "\n",
        "            # Number of ticks this minute (Poisson distributed)\n",
        "            n_ticks = max(1, int(np.random.poisson(5 * base_volume_curve[minute])))\n",
        "\n",
        "            for tick in range(n_ticks):\n",
        "                # Price movement (random walk with mean reversion)\n",
        "                price_change = np.random.normal(0, params['volatility'] * params['price'] / 100)\n",
        "                current_price += price_change\n",
        "                current_price = max(current_price, params['price'] * 0.95)  # Floor\n",
        "                current_price = min(current_price, params['price'] * 1.05)  # Ceiling\n",
        "\n",
        "                # Spread (wider during low volume)\n",
        "                spread_multiplier = 1.0 + (1.5 - base_volume_curve[minute])\n",
        "                spread = params['price'] * params['spread_bps'] / 10000 * spread_multiplier\n",
        "                spread = max(spread, 0.01)\n",
        "\n",
        "                mid_price = current_price\n",
        "                bid = mid_price - spread / 2\n",
        "                ask = mid_price + spread / 2\n",
        "\n",
        "                # Volume (log-normal distribution)\n",
        "                volume = int(np.random.lognormal(\n",
        "                    np.log(params['avg_volume'] / n_minutes / 5),\n",
        "                    0.8\n",
        "                ) * base_volume_curve[minute])\n",
        "                volume = max(volume, 100)\n",
        "\n",
        "                # Trade direction (slight buy pressure on trade days during window)\n",
        "                if is_trade_day and trade_start_idx <= minute < trade_end_idx:\n",
        "                    trade_direction = np.random.choice([1, -1], p=[0.55, 0.45])  # 55% buy\n",
        "                else:\n",
        "                    trade_direction = np.random.choice([1, -1], p=[0.50, 0.50])  # 50/50\n",
        "\n",
        "                # Execution price (between bid/ask)\n",
        "                if trade_direction == 1:  # Buy\n",
        "                    price = ask - np.random.uniform(0, spread * 0.3)\n",
        "                else:  # Sell\n",
        "                    price = bid + np.random.uniform(0, spread * 0.3)\n",
        "\n",
        "                # Depth (inverse relationship with volume)\n",
        "                depth_multiplier = 2.0 - base_volume_curve[minute]\n",
        "                bid_size = int(np.random.lognormal(8, 1) * depth_multiplier)\n",
        "                ask_size = int(np.random.lognormal(8, 1) * depth_multiplier)\n",
        "\n",
        "                # Order book imbalance (if trade day, slight bid-side pressure)\n",
        "                if is_trade_day and trade_start_idx <= minute < trade_end_idx:\n",
        "                    bid_size = int(bid_size * 1.15)  # More bid depth\n",
        "\n",
        "                # Quote updates (more during volatile periods)\n",
        "                quote_update = 1 if np.random.random() < 0.3 else 0\n",
        "\n",
        "                tick_data = {\n",
        "                    'symbol': symbol,\n",
        "                    'date': date,\n",
        "                    'timestamp': timestamp + timedelta(seconds=tick * (60 / n_ticks)),\n",
        "                    'price': round(price, 2),\n",
        "                    'volume': volume,\n",
        "                    'bid': round(bid, 2),\n",
        "                    'ask': round(ask, 2),\n",
        "                    'bid_size': bid_size,\n",
        "                    'ask_size': ask_size,\n",
        "                    'trade_direction': trade_direction,\n",
        "                    'quote_update': quote_update\n",
        "                }\n",
        "\n",
        "                ticks.append(tick_data)\n",
        "\n",
        "        return pd.DataFrame(ticks)\n",
        "\n",
        "    def generate_multi_day_data(self, symbols, start_date, n_days=30,\n",
        "                                trade_days_per_symbol=None):\n",
        "        \"\"\"\n",
        "        Generate multiple days for multiple symbols\n",
        "\n",
        "        trade_days_per_symbol: Dict[symbol, List[date]] - which days you traded each symbol\n",
        "        \"\"\"\n",
        "        if trade_days_per_symbol is None:\n",
        "            trade_days_per_symbol = {}\n",
        "\n",
        "        all_data = []\n",
        "        dates = [start_date + timedelta(days=i) for i in range(n_days)]\n",
        "\n",
        "        # Filter out weekends\n",
        "        dates = [d for d in dates if d.weekday() < 5]\n",
        "\n",
        "        for symbol in symbols:\n",
        "            symbol_trade_days = trade_days_per_symbol.get(symbol, set())\n",
        "\n",
        "            for date in dates:\n",
        "                is_trade_day = date in symbol_trade_days\n",
        "\n",
        "                # Vary trade intensity across days\n",
        "                if is_trade_day:\n",
        "                    trade_intensity = np.random.uniform(1.3, 1.8)\n",
        "                    trade_start = np.random.uniform(0.2, 0.4)\n",
        "                    trade_end = np.random.uniform(0.6, 0.8)\n",
        "                else:\n",
        "                    trade_intensity = 1.0\n",
        "                    trade_start = 0.3\n",
        "                    trade_end = 0.7\n",
        "\n",
        "                day_data = self.generate_day_data(\n",
        "                    symbol, date, is_trade_day,\n",
        "                    trade_intensity, trade_start, trade_end\n",
        "                )\n",
        "                all_data.append(day_data)\n",
        "\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# FOOTPRINT ANALYZER (from previous code)\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class FootprintMetrics:\n",
        "    \"\"\"All metrics for a single time bucket\"\"\"\n",
        "    volume: float\n",
        "    trade_count: int\n",
        "    vwap: float\n",
        "    price_return: float\n",
        "    volatility: float\n",
        "    spread: float\n",
        "    spread_pct: float\n",
        "    bid_depth: float\n",
        "    ask_depth: float\n",
        "    depth_imbalance: float\n",
        "    trade_imbalance: float\n",
        "    quote_intensity: int\n",
        "    price_impact: float\n",
        "    effective_spread: float\n",
        "    realized_spread: float\n",
        "\n",
        "\n",
        "class ComprehensiveFootprintAnalyzer:\n",
        "    def __init__(self, lookback_days=21, bucket_minutes=1):\n",
        "        self.lookback_days = lookback_days\n",
        "        self.bucket_minutes = bucket_minutes\n",
        "\n",
        "        self.metric_groups = {\n",
        "            'volume_metrics': ['volume', 'trade_count', 'trade_imbalance'],\n",
        "            'price_metrics': ['vwap', 'price_return', 'price_impact'],\n",
        "            'volatility_metrics': ['volatility', 'realized_spread'],\n",
        "            'liquidity_metrics': ['spread', 'spread_pct', 'effective_spread'],\n",
        "            'depth_metrics': ['bid_depth', 'ask_depth', 'depth_imbalance'],\n",
        "            'activity_metrics': ['quote_intensity']\n",
        "        }\n",
        "\n",
        "        self.metric_weights = {\n",
        "            'volume': 0.20,\n",
        "            'trade_count': 0.10,\n",
        "            'price_return': 0.15,\n",
        "            'volatility': 0.15,\n",
        "            'spread': 0.10,\n",
        "            'depth_imbalance': 0.10,\n",
        "            'trade_imbalance': 0.10,\n",
        "            'price_impact': 0.10\n",
        "        }\n",
        "\n",
        "    def compute_bucket_metrics(self, bucket_data, prev_bucket_data=None):\n",
        "        \"\"\"Compute all metrics for a single time bucket\"\"\"\n",
        "        if len(bucket_data) == 0:\n",
        "            return None\n",
        "\n",
        "        metrics = {}\n",
        "\n",
        "        # Volume metrics\n",
        "        metrics['volume'] = bucket_data['volume'].sum()\n",
        "        metrics['trade_count'] = len(bucket_data)\n",
        "        metrics['vwap'] = (bucket_data['price'] * bucket_data['volume']).sum() / metrics['volume'] if metrics['volume'] > 0 else bucket_data['price'].mean()\n",
        "\n",
        "        # Trade direction imbalance\n",
        "        if 'trade_direction' in bucket_data.columns:\n",
        "            buy_vol = bucket_data[bucket_data['trade_direction'] == 1]['volume'].sum()\n",
        "            sell_vol = bucket_data[bucket_data['trade_direction'] == -1]['volume'].sum()\n",
        "            total_vol = buy_vol + sell_vol\n",
        "            metrics['trade_imbalance'] = (buy_vol - sell_vol) / total_vol if total_vol > 0 else 0\n",
        "        else:\n",
        "            metrics['trade_imbalance'] = 0\n",
        "\n",
        "        # Price metrics\n",
        "        if prev_bucket_data is not None and len(prev_bucket_data) > 0:\n",
        "            prev_vwap = (prev_bucket_data['price'] * prev_bucket_data['volume']).sum() / prev_bucket_data['volume'].sum()\n",
        "            metrics['price_return'] = (metrics['vwap'] - prev_vwap) / prev_vwap if prev_vwap > 0 else 0\n",
        "        else:\n",
        "            metrics['price_return'] = 0\n",
        "\n",
        "        metrics['volatility'] = bucket_data['price'].std() if len(bucket_data) > 1 else 0\n",
        "\n",
        "        # Spread metrics\n",
        "        if 'bid' in bucket_data.columns and 'ask' in bucket_data.columns:\n",
        "            metrics['spread'] = (bucket_data['ask'] - bucket_data['bid']).mean()\n",
        "            mid_price = (bucket_data['bid'] + bucket_data['ask']) / 2\n",
        "            metrics['spread_pct'] = (metrics['spread'] / mid_price.mean()) * 100 if mid_price.mean() > 0 else 0\n",
        "            metrics['effective_spread'] = 2 * np.abs(bucket_data['price'] - mid_price).mean()\n",
        "        else:\n",
        "            metrics['spread'] = 0\n",
        "            metrics['spread_pct'] = 0\n",
        "            metrics['effective_spread'] = 0\n",
        "\n",
        "        # Depth metrics\n",
        "        if 'bid_size' in bucket_data.columns and 'ask_size' in bucket_data.columns:\n",
        "            metrics['bid_depth'] = bucket_data['bid_size'].mean()\n",
        "            metrics['ask_depth'] = bucket_data['ask_size'].mean()\n",
        "            total_depth = metrics['bid_depth'] + metrics['ask_depth']\n",
        "            metrics['depth_imbalance'] = (metrics['bid_depth'] - metrics['ask_depth']) / total_depth if total_depth > 0 else 0\n",
        "        else:\n",
        "            metrics['bid_depth'] = 0\n",
        "            metrics['ask_depth'] = 0\n",
        "            metrics['depth_imbalance'] = 0\n",
        "\n",
        "        # Quote intensity\n",
        "        if 'quote_update' in bucket_data.columns:\n",
        "            metrics['quote_intensity'] = bucket_data['quote_update'].sum()\n",
        "        else:\n",
        "            metrics['quote_intensity'] = 0\n",
        "\n",
        "        # Price impact\n",
        "        if metrics['volume'] > 0 and len(bucket_data) > 1:\n",
        "            price_move = np.abs(bucket_data['price'].iloc[-1] - bucket_data['price'].iloc[0])\n",
        "            metrics['price_impact'] = (price_move / bucket_data['price'].iloc[0]) / (metrics['volume'] / 1e6) if bucket_data['price'].iloc[0] > 0 else 0\n",
        "        else:\n",
        "            metrics['price_impact'] = 0\n",
        "\n",
        "        metrics['realized_spread'] = 0\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def create_comprehensive_baseline(self, historical_data, trade_dates, current_date, symbol):\n",
        "        \"\"\"Create baseline profile with ALL metrics\"\"\"\n",
        "        # Filter for this symbol\n",
        "        symbol_data = historical_data[historical_data['symbol'] == symbol].copy()\n",
        "\n",
        "        # Get prior dates excluding trading days\n",
        "        all_dates = sorted(symbol_data['date'].unique())\n",
        "        all_dates = [d for d in all_dates if d < current_date]\n",
        "\n",
        "        non_trade_dates = [d for d in all_dates if d not in trade_dates]\n",
        "        baseline_dates = non_trade_dates[-self.lookback_days:]\n",
        "\n",
        "        if len(baseline_dates) < self.lookback_days:\n",
        "            print(f\"âš ï¸ Warning: Only {len(baseline_dates)} non-trading days available for {symbol}\")\n",
        "\n",
        "        baseline_data = symbol_data[symbol_data['date'].isin(baseline_dates)].copy()\n",
        "\n",
        "        # Create time buckets\n",
        "        baseline_data['time_bucket'] = pd.to_datetime(\n",
        "            baseline_data['timestamp']\n",
        "        ).dt.floor(f'{self.bucket_minutes}min').dt.time\n",
        "\n",
        "        # Compute metrics for each date and time bucket\n",
        "        baseline_profiles = []\n",
        "\n",
        "        for date in baseline_dates:\n",
        "            date_data = baseline_data[baseline_data['date'] == date]\n",
        "            time_buckets = sorted(date_data['time_bucket'].unique())\n",
        "\n",
        "            prev_bucket_data = None\n",
        "            for tb in time_buckets:\n",
        "                bucket_data = date_data[date_data['time_bucket'] == tb]\n",
        "                metrics = self.compute_bucket_metrics(bucket_data, prev_bucket_data)\n",
        "\n",
        "                if metrics:\n",
        "                    metrics['date'] = date\n",
        "                    metrics['time_bucket'] = tb\n",
        "                    baseline_profiles.append(metrics)\n",
        "\n",
        "                prev_bucket_data = bucket_data\n",
        "\n",
        "        baseline_df = pd.DataFrame(baseline_profiles)\n",
        "\n",
        "        if len(baseline_df) == 0:\n",
        "            return None, None\n",
        "\n",
        "        # Average across all baseline dates\n",
        "        baseline_avg = baseline_df.groupby('time_bucket').agg({\n",
        "            metric: ['mean', 'std', 'median']\n",
        "            for metric in self.metric_weights.keys()\n",
        "        }).reset_index()\n",
        "\n",
        "        # Flatten column names\n",
        "        baseline_avg.columns = ['_'.join(col).strip('_') for col in baseline_avg.columns.values]\n",
        "\n",
        "        return baseline_avg, baseline_df\n",
        "\n",
        "    def compute_trade_day_profile(self, trade_day_data):\n",
        "        \"\"\"Compute full metric profile for trade day\"\"\"\n",
        "        trade_day_data = trade_day_data.copy()\n",
        "        trade_day_data['time_bucket'] = pd.to_datetime(\n",
        "            trade_day_data['timestamp']\n",
        "        ).dt.floor(f'{self.bucket_minutes}min').dt.time\n",
        "\n",
        "        time_buckets = sorted(trade_day_data['time_bucket'].unique())\n",
        "        trade_profiles = []\n",
        "\n",
        "        prev_bucket_data = None\n",
        "        for tb in time_buckets:\n",
        "            bucket_data = trade_day_data[trade_day_data['time_bucket'] == tb]\n",
        "            metrics = self.compute_bucket_metrics(bucket_data, prev_bucket_data)\n",
        "\n",
        "            if metrics:\n",
        "                metrics['time_bucket'] = tb\n",
        "                trade_profiles.append(metrics)\n",
        "\n",
        "            prev_bucket_data = bucket_data\n",
        "\n",
        "        return pd.DataFrame(trade_profiles)\n",
        "\n",
        "    def compute_metric_similarity(self, trade_values, baseline_mean, baseline_std):\n",
        "        \"\"\"Compute multiple similarity measures for a single metric\"\"\"\n",
        "        # Normalize\n",
        "        trade_norm = trade_values / (trade_values.sum() + 1e-10)\n",
        "        baseline_norm = baseline_mean / (baseline_mean.sum() + 1e-10)\n",
        "\n",
        "        similarity_scores = {}\n",
        "\n",
        "        # Correlation\n",
        "        if len(trade_norm) > 1 and len(baseline_norm) > 1:\n",
        "            similarity_scores['pearson'] = stats.pearsonr(trade_norm, baseline_norm)[0]\n",
        "            similarity_scores['spearman'] = stats.spearmanr(trade_norm, baseline_norm)[0]\n",
        "        else:\n",
        "            similarity_scores['pearson'] = np.nan\n",
        "            similarity_scores['spearman'] = np.nan\n",
        "\n",
        "        # Distance metrics\n",
        "        similarity_scores['euclidean'] = euclidean(trade_norm, baseline_norm)\n",
        "        similarity_scores['cosine_sim'] = 1 - cosine(trade_norm, baseline_norm)\n",
        "\n",
        "        # KS test\n",
        "        ks_stat, ks_pval = stats.ks_2samp(trade_values, baseline_mean)\n",
        "        similarity_scores['ks_statistic'] = ks_stat\n",
        "        similarity_scores['ks_pvalue'] = ks_pval\n",
        "\n",
        "        # KL divergence\n",
        "        similarity_scores['kl_divergence'] = stats.entropy(\n",
        "            trade_norm + 1e-10,\n",
        "            baseline_norm + 1e-10\n",
        "        )\n",
        "\n",
        "        # Z-scores\n",
        "        if baseline_std is not None and (baseline_std > 0).any():\n",
        "            z_scores = (trade_values - baseline_mean) / (baseline_std + 1e-10)\n",
        "            similarity_scores['mean_z_score'] = np.abs(z_scores).mean()\n",
        "            similarity_scores['max_z_score'] = np.abs(z_scores).max()\n",
        "        else:\n",
        "            similarity_scores['mean_z_score'] = np.nan\n",
        "            similarity_scores['max_z_score'] = np.nan\n",
        "\n",
        "        # Wasserstein\n",
        "        similarity_scores['wasserstein'] = stats.wasserstein_distance(trade_norm, baseline_norm)\n",
        "\n",
        "        return similarity_scores\n",
        "\n",
        "    def analyze_full_footprint(self, trade_day_profile, baseline_avg):\n",
        "        \"\"\"Compare trade day vs baseline across ALL metrics\"\"\"\n",
        "        # Merge on time_bucket\n",
        "        merged = trade_day_profile.merge(\n",
        "            baseline_avg,\n",
        "            on='time_bucket',\n",
        "            suffixes=('_trade', '_baseline')\n",
        "        )\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Analyze each metric\n",
        "        for metric in self.metric_weights.keys():\n",
        "            trade_col = f'{metric}_trade' if f'{metric}_trade' in merged.columns else metric\n",
        "            baseline_mean_col = f'{metric}_mean'\n",
        "            baseline_std_col = f'{metric}_std'\n",
        "\n",
        "            if trade_col in merged.columns and baseline_mean_col in merged.columns:\n",
        "                trade_values = merged[trade_col].values\n",
        "                baseline_mean = merged[baseline_mean_col].values\n",
        "                baseline_std = merged[baseline_std_col].values if baseline_std_col in merged.columns else None\n",
        "\n",
        "                results[metric] = self.compute_metric_similarity(\n",
        "                    trade_values, baseline_mean, baseline_std\n",
        "                )\n",
        "\n",
        "        return results, merged\n",
        "\n",
        "    def compute_composite_score(self, metric_results):\n",
        "        \"\"\"Aggregate similarity across all metrics\"\"\"\n",
        "        composite_scores = {\n",
        "            'weighted_correlation': 0,\n",
        "            'weighted_distance': 0,\n",
        "            'weighted_ks_pval': 0,\n",
        "            'suspicious_metrics': []\n",
        "        }\n",
        "\n",
        "        total_weight = 0\n",
        "\n",
        "        for metric, weight in self.metric_weights.items():\n",
        "            if metric in metric_results:\n",
        "                result = metric_results[metric]\n",
        "\n",
        "                # Correlation component\n",
        "                if not np.isnan(result.get('pearson', np.nan)):\n",
        "                    composite_scores['weighted_correlation'] += weight * result['pearson']\n",
        "                    total_weight += weight\n",
        "\n",
        "                # Distance component\n",
        "                eucl = result.get('euclidean', 0)\n",
        "                composite_scores['weighted_distance'] += weight * (1 / (1 + eucl))\n",
        "\n",
        "                # Statistical significance\n",
        "                ks_pval = result.get('ks_pvalue', 1)\n",
        "                composite_scores['weighted_ks_pval'] += weight * ks_pval\n",
        "\n",
        "                # Flag suspicious metrics\n",
        "                if result.get('pearson', 1) < 0.7 or result.get('ks_pvalue', 1) < 0.05:\n",
        "                    composite_scores['suspicious_metrics'].append({\n",
        "                        'metric': metric,\n",
        "                        'correlation': result.get('pearson', np.nan),\n",
        "                        'ks_pvalue': result.get('ks_pvalue', np.nan),\n",
        "                        'mean_z_score': result.get('mean_z_score', np.nan)\n",
        "                    })\n",
        "\n",
        "        # Normalize\n",
        "        if total_weight > 0:\n",
        "            composite_scores['weighted_correlation'] /= total_weight\n",
        "\n",
        "        # Detectability score (0-100)\n",
        "        detectability = 100 * (1 - composite_scores['weighted_correlation'])\n",
        "        composite_scores['detectability_score'] = detectability\n",
        "\n",
        "        return composite_scores\n",
        "\n",
        "    def generate_report(self, metric_results, composite_scores, symbol, trade_date):\n",
        "        \"\"\"Generate human-readable report\"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"VWAP FOOTPRINT ANALYSIS - {symbol} - {trade_date}\")\n",
        "        print(\"=\" * 80)\n",
        "        print()\n",
        "\n",
        "        print(f\"ðŸ“Š COMPOSITE DETECTABILITY SCORE: {composite_scores['detectability_score']:.1f}/100\")\n",
        "        print()\n",
        "\n",
        "        if composite_scores['detectability_score'] < 20:\n",
        "            print(\"âœ… EXCELLENT: Your footprint blends in very well with normal market activity\")\n",
        "        elif composite_scores['detectability_score'] < 40:\n",
        "            print(\"âœ“ GOOD: Footprint is reasonably well disguised\")\n",
        "        elif composite_scores['detectability_score'] < 60:\n",
        "            print(\"âš ï¸ MODERATE: Some detectability - consider randomization\")\n",
        "        else:\n",
        "            print(\"ðŸš¨ HIGH RISK: Footprint is highly detectable - likely exploitable!\")\n",
        "\n",
        "        print()\n",
        "        print(\"-\" * 80)\n",
        "        print(\"METRIC-BY-METRIC ANALYSIS\")\n",
        "        print(\"-\" * 80)\n",
        "        print()\n",
        "\n",
        "        # Sort metrics by correlation\n",
        "        sorted_metrics = sorted(\n",
        "            metric_results.items(),\n",
        "            key=lambda x: x[1].get('pearson', 0)\n",
        "        )\n",
        "\n",
        "        for metric, scores in sorted_metrics:\n",
        "            corr = scores.get('pearson', np.nan)\n",
        "            ks_pval = scores.get('ks_pvalue', np.nan)\n",
        "            mean_z = scores.get('mean_z_score', np.nan)\n",
        "\n",
        "            status = \"âœ…\" if corr > 0.8 else \"âš ï¸\" if corr > 0.6 else \"ðŸš¨\"\n",
        "\n",
        "            print(f\"{status} {metric.upper()}\")\n",
        "            print(f\"   Correlation:    {corr:.3f}\")\n",
        "            print(f\"   KS p-value:     {ks_pval:.4f}\")\n",
        "            print(f\"   Mean Z-score:   {mean_z:.2f}\")\n",
        "            print(f\"   Euclidean dist: {scores.get('euclidean', np.nan):.4f}\")\n",
        "            print()\n",
        "\n",
        "        if composite_scores['suspicious_metrics']:\n",
        "            print(\"-\" * 80)\n",
        "            print(\"ðŸ” SUSPICIOUS METRICS (requiring attention)\")\n",
        "            print(\"-\" * 80)\n",
        "            for item in composite_scores['suspicious_metrics']:\n",
        "                print(f\"  â€¢ {item['metric']}: correlation={item['correlation']:.3f}, \"\n",
        "                      f\"p-value={item['ks_pvalue']:.4f}\")\n",
        "            print()\n",
        "\n",
        "        print(\"=\" * 80)\n",
        "        print()\n",
        "\n",
        "\n",
        "def plot_comprehensive_footprint(merged_data, metric_results, symbol, trade_date):\n",
        "    \"\"\"Create comprehensive visualization\"\"\"\n",
        "    key_metrics = ['volume', 'volatility', 'spread', 'depth_imbalance',\n",
        "                   'trade_imbalance', 'price_impact']\n",
        "\n",
        "    n_metrics = len(key_metrics)\n",
        "    fig, axes = plt.subplots(n_metrics, 2, figsize=(16, 4*n_metrics))\n",
        "\n",
        "    for idx, metric in enumerate(key_metrics):\n",
        "        # Left: Time series comparison\n",
        "        ax1 = axes[idx, 0]\n",
        "        x = range(len(merged_data))\n",
        "\n",
        "        trade_col = f'{metric}_trade' if f'{metric}_trade' in merged_data.columns else metric\n",
        "        baseline_col = f'{metric}_mean'\n",
        "        std_col = f'{metric}_std'\n",
        "\n",
        "        if baseline_col in merged_data.columns:\n",
        "            ax1.plot(x, merged_data[baseline_col],\n",
        "                    label='Baseline (21-day avg)', linewidth=2, alpha=0.7, color='blue')\n",
        "            if std_col in merged_data.columns:\n",
        "                ax1.fill_between(x,\n",
        "                               merged_data[baseline_col] - merged_data[std_col],\n",
        "                               merged_data[baseline_col] + merged_data[std_col],\n",
        "                               alpha=0.2, color='blue')\n",
        "\n",
        "        if trade_col in merged_data.columns:\n",
        "            ax1.plot(x, merged_data[trade_col],\n",
        "                    label=f'Trade Day', linewidth=2, alpha=0.8, color='red')\n",
        "\n",
        "        # Add correlation score\n",
        "        if metric in metric_results:\n",
        "            corr = metric_results[metric].get('pearson', np.nan)\n",
        "            ax1.text(0.02, 0.98, f'Correlation: {corr:.3f}',\n",
        "                    transform=ax1.transAxes, fontsize=10,\n",
        "                    verticalalignment='top',\n",
        "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "        ax1.set_title(f'{metric.replace(\"_\", \" \").title()} - Time Series')\n",
        "        ax1.set_xlabel('Time Bucket (minutes)')\n",
        "        ax1.set_ylabel(metric)\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Right: Distribution comparison\n",
        "        ax2 = axes[idx, 1]\n",
        "\n",
        "        if baseline_col in merged_data.columns and trade_col in merged_data.columns:\n",
        "            ax2.hist(merged_data[baseline_col], bins=20, alpha=0.5,\n",
        "                    label='Baseline', color='blue', density=True)\n",
        "            ax2.hist(merged_data[trade_col], bins=20, alpha=0.5,\n",
        "                    label='Trade Day', color='red', density=True)\n",
        "\n",
        "            # Add KS test result\n",
        "            if metric in metric_results:\n",
        "                ks_pval = metric_results[metric].get('ks_pvalue', np.nan)\n",
        "                ax2.text(0.02, 0.98, f'KS p-value: {ks_pval:.4f}',\n",
        "                        transform=ax2.transAxes, fontsize=10,\n",
        "                        verticalalignment='top',\n",
        "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "        ax2.set_title(f'{metric.replace(\"_\", \" \").title()} - Distribution')\n",
        "        ax2.set_xlabel(metric)\n",
        "        ax2.set_ylabel('Density')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.suptitle(f'Comprehensive Footprint Analysis - {symbol} - {trade_date}',\n",
        "                 fontsize=16, y=1.001)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_multi_symbol_heatmap(all_results):\n",
        "    \"\"\"\n",
        "    Heatmap showing detectability across symbols and dates\n",
        "\n",
        "    all_results: Dict[(symbol, date), composite_scores]\n",
        "    \"\"\"\n",
        "    # Extract data for heatmap\n",
        "    symbols = sorted(set(k[0] for k in all_results.keys()))\n",
        "    dates = sorted(set(k[1] for k in all_results.keys()))\n",
        "\n",
        "    # Create matrix\n",
        "    matrix = np.zeros((len(symbols), len(dates)))\n",
        "\n",
        "    for i, symbol in enumerate(symbols):\n",
        "        for j, date in enumerate(dates):\n",
        "            key = (symbol, date)\n",
        "            if key in all_results:\n",
        "                matrix[i, j] = all_results[key]['detectability_score']\n",
        "            else:\n",
        "                matrix[i, j] = np.nan\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(max(12, len(dates)*0.8), max(6, len(symbols)*0.6)))\n",
        "\n",
        "    # Create mask for nan values\n",
        "    mask = np.isnan(matrix)\n",
        "\n",
        "    sns.heatmap(matrix,\n",
        "                xticklabels=[d.strftime('%m/%d') for d in dates],\n",
        "                yticklabels=symbols,\n",
        "                annot=True,\n",
        "                fmt='.1f',\n",
        "                cmap='RdYlGn_r',  # Red = high detectability (bad), Green = low (good)\n",
        "                center=50,\n",
        "                vmin=0, vmax=100,\n",
        "                mask=mask,\n",
        "                ax=ax,\n",
        "                cbar_kws={'label': 'Detectability Score (0-100)'})\n",
        "\n",
        "    ax.set_title('Multi-Symbol Footprint Detectability Heatmap\\nGreen=Well Disguised, Red=Highly Detectable')\n",
        "    ax.set_xlabel('Trade Date')\n",
        "    ax.set_ylabel('Symbol')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# COMPLETE WORKING EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"Generating mock market data...\")\n",
        "    print()\n",
        "\n",
        "    # Initialize data generator\n",
        "    generator = MarketDataGenerator(seed=42)\n",
        "\n",
        "    # Define symbols to analyze\n",
        "    symbols = ['AAPL', 'MSFT', 'GOOGL', 'TSLA']\n",
        "\n",
        "    # Define trading schedule\n",
        "    start_date = datetime(2024, 10, 1).date()\n",
        "    n_days = 35\n",
        "\n",
        "    # Define which days you traded each symbol\n",
        "    all_dates = [start_date + timedelta(days=i) for i in range(n_days)]\n",
        "    all_dates = [d for d in all_dates if d.weekday() < 5]  # Remove weekends\n",
        "\n",
        "    # Pick some random trade dates for each symbol\n",
        "    np.random.seed(42)\n",
        "    trade_days_per_symbol = {\n",
        "        'AAPL': set(np.random.choice(all_dates[-15:], size=5, replace=False)),\n",
        "        'MSFT': set(np.random.choice(all_dates[-15:], size=4, replace=False)),\n",
        "        'GOOGL': set(np.random.choice(all_dates[-15:], size=6, replace=False)),\n",
        "        'TSLA': set(np.random.choice(all_dates[-15:], size=3, replace=False)),\n",
        "    }\n",
        "\n",
        "    # Generate data\n",
        "    market_data = generator.generate_multi_day_data(\n",
        "        symbols=symbols,\n",
        "        start_date=start_date,\n",
        "        n_days=n_days,\n",
        "        trade_days_per_symbol=trade_days_per_symbol\n",
        "    )\n",
        "\n",
        "    print(f\"âœ“ Generated {len(market_data):,} ticks across {len(symbols)} symbols and {len(all_dates)} days\")\n",
        "    print(f\"âœ“ Data shape: {market_data.shape}\")\n",
        "    print(f\"âœ“ Date range: {market_data['date'].min()} to {market_data['date'].max()}\")\n",
        "    print()\n",
        "\n",
        "    # Display sample\n",
        "    print(\"Sample data:\")\n",
        "    print(market_data.head(10))\n",
        "    print()\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = ComprehensiveFootprintAnalyzer(lookback_days=15, bucket_minutes=5)\n",
        "\n",
        "    # Run analysis for each symbol on their trade days\n",
        "    all_results = {}\n",
        "\n",
        "    for symbol in symbols:\n",
        "        symbol_trade_dates = trade_days_per_symbol[symbol]\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ANALYZING {symbol} - {len(symbol_trade_dates)} trade days\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        for trade_date in sorted(symbol_trade_dates):\n",
        "            try:\n",
        "                # Create baseline\n",
        "                baseline_avg, baseline_full = analyzer.create_comprehensive_baseline(\n",
        "                    historical_data=market_data,\n",
        "                    trade_dates=symbol_trade_dates,\n",
        "                    current_date=trade_date,\n",
        "                    symbol=symbol\n",
        "                )\n",
        "\n",
        "                if baseline_avg is None:\n",
        "                    print(f\"âš ï¸ Skipping {symbol} {trade_date} - insufficient baseline data\")\n",
        "                    continue\n",
        "\n",
        "                # Get trade day data\n",
        "                trade_day_data = market_data[\n",
        "                    (market_data['symbol'] == symbol) &\n",
        "                    (market_data['date'] == trade_date)\n",
        "                ]\n",
        "\n",
        "                if len(trade_day_data) == 0:\n",
        "                    print(f\"âš ï¸ No data for {symbol} on {trade_date}\")\n",
        "                    continue\n",
        "\n",
        "                # Compute trade day profile\n",
        "                trade_profile = analyzer.compute_trade_day_profile(trade_day_data)\n",
        "\n",
        "                # Analyze footprint\n",
        "                metric_results, merged_data = analyzer.analyze_full_footprint(\n",
        "                    trade_profile, baseline_avg\n",
        "                )\n",
        "\n",
        "                # Compute composite score\n",
        "                composite_scores = analyzer.compute_composite_score(metric_results)\n",
        "\n",
        "                # Store results\n",
        "                all_results[(symbol, trade_date)] = composite_scores\n",
        "\n",
        "                # Generate report\n",
        "                analyzer.generate_report(metric_results, composite_scores, symbol, trade_date)\n",
        "\n",
        "                # Create visualization\n",
        "                fig = plot_comprehensive_footprint(merged_data, metric_results, symbol, trade_date)\n",
        "                plt.savefig(f'footprint_{symbol}_{trade_date}.png', dpi=100, bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error analyzing {symbol} {trade_date}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "    # Create summary heatmap\n",
        "    if all_results:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"GENERATING MULTI-SYMBOL SUMMARY\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        fig = plot_multi_symbol_heatmap(all_results)\n",
        "        plt.savefig('footprint_summary_heatmap.png', dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        # Summary statistics\n",
        "        print(\"\\nSUMMARY STATISTICS:\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        for symbol in symbols:\n",
        "            symbol_scores = [v['detectability_score'] for k, v in all_results.items() if k[0] == symbol]\n",
        "            if symbol_scores:\n",
        "                print(f\"\\n{symbol}:\")\n",
        "                print(f\"  Mean detectability:   {np.mean(symbol_scores):.1f}\")\n",
        "                print(f\"  Min detectability:    {np.min(symbol_scores):.1f}\")\n",
        "                print(f\"  Max detectability:    {np.max(symbol_scores):.1f}\")\n",
        "                print(f\"  Std deviation:        {np.std(symbol_scores):.1f}\")\n",
        "\n",
        "                high_risk_days = sum(1 for s in symbol_scores if s > 60)\n",
        "                if high_risk_days > 0:\n",
        "                    print(f\"  ðŸš¨ High risk days:    {high_risk_days}/{len(symbol_scores)}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ANALYSIS COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nâœ“ Analyzed {len(all_results)} symbol-date combinations\")\n",
        "    print(f\"âœ“ Generated {len(all_results)} individual charts\")\n",
        "    print(f\"âœ“ Generated 1 summary heatmap\")\n",
        "    print(\"\\nFiles created:\")\n",
        "    print(\"  - footprint_<SYMBOL>_<DATE>.png (individual analyses)\")\n",
        "    print(\"  - footprint_summary_heatmap.png (overview)\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}